# E-commerce Demand Forecasting with PySpark (Spark SQL & ML)

This repo contains a **production-style PySpark project** that forecasts daily product demand for an e-commerce retailer using the classic **Online Retail** dataset.

The goal is to move beyond "just notebooks" into a **software-engineering style ML project** with:

- Clear folder structure
- Reusable modules under `src/`
- Config-driven experiments (`configs/*.yaml`)
- CLI scripts under `scripts/`
- Model artifacts in `models/`

---

## ðŸš€ Project Goal & Business Context

You are a data scientist for the **Sales & Operations Planning (S&OP)** team at a multinational e-commerce company.

- **Problem:** Uncertain demand leads to poor inventory planning.  
  - Overstock â†’ cash tied in inventory, markdowns  
  - Understock â†’ lost sales, angry customers  
- **Solution:** Build a demand forecasting model to predict **daily quantities sold** by product and country.

This project forecasts **daily `TotalQuantity`** for each `(StockCode, Country)` combination.

---

## ðŸ§± Project Structure

```bash
ecommerce-demand-forecast-pyspark/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ spark.yaml             # Spark session config (master, memory, etc.)
â”‚   â”œâ”€â”€ data.yaml              # Data paths and column names
â”‚   â””â”€â”€ model.yaml             # Pipeline, model, and training config
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw source data (e.g., Online Retail.csv)
â”‚   â”œâ”€â”€ interim/               # Optional intermediate tables
â”‚   â””â”€â”€ processed/             # Final ML-ready tables
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ spark-demand-forecast.ipynb   # Exploration & scratch work
â”œâ”€â”€ src/
â”‚   â””â”€â”€ demand_forecast/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ spark_session.py          # SparkSession factory (config-driven)
â”‚       â”œâ”€â”€ data_ingestion.py         # Load raw data
â”‚       â”œâ”€â”€ cleaning.py               # Data cleaning rules
â”‚       â”œâ”€â”€ features.py               # Aggregation & feature engineering
â”‚       â”œâ”€â”€ modeling.py               # Pipeline + model training
â”‚       â”œâ”€â”€ evaluation.py             # Metrics (RMSE, MAE, R2, SMAPE)
â”‚       â”œâ”€â”€ plotting.py               # EDA & diagnostic plots
â”‚       â””â”€â”€ io_utils.py               # Model saving helpers
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_model.py                # Full training + evaluation + save
â”‚   â””â”€â”€ run_eda.py                    # Quick EDA plots
â”œâ”€â”€ models/
â”‚   â””â”€â”€ rf_baseline/                  # Saved Spark PipelineModel
â””â”€â”€ tests/
    â””â”€â”€ ...                           # (Optional) unit tests
